{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a093b1",
   "metadata": {},
   "source": [
    "# ENSAMBLE METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c6951",
   "metadata": {},
   "source": [
    "ensemble method is a technique which uses multiple independent similar or different models/weak learners to derive an output or make some predictions. For e.g. A random forest is an ensemble of multiple decision trees.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e45638",
   "metadata": {},
   "source": [
    "ensamble methds work dicision tree model by defalut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da43f1",
   "metadata": {},
   "source": [
    "two ensable technique\n",
    "\n",
    "1 .BOGGING(paraller )-use random farest\n",
    "\n",
    "Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once. After several data samples are generated, these weak models are then trained independently, and depending on the type of task—regression or classification, for example—the average or majority of those predictions yield a more accurate estimate. \n",
    "\n",
    "As a note, the random forest algorithm is considered an extension of the bagging method, using both bagging and feature randomness to create an uncorrelated forest of decision trees.\n",
    "\n",
    "2. Boosting(serieal )- ue ada boosting and graidient boosting\n",
    "\n",
    "difference between these learning methods is the way in which they are trained. In bagging, weak learners are trained in parallel, but in boosting, they learn sequentially. This means that a series of models are constructed and with each new model iteration, the weights of the misclassified data in the previous model are increased. This redistribution of weights helps the algorithm identify the parameters that it needs to focus on to improve its performance. AdaBoost, which stands for “adaptative boosting algorithm,” is one of the most popular boosting algorithms as it was one of the first of its kind. Other types of boosting algorithms include XGBoost, GradientBoost, and BrownBoost.\n",
    "\n",
    "Another difference in which bagging and boosting differ are the scenarios in which they are used. For example, bagging methods are typically used on weak learners which exhibit high variance and low bias, whereas boosting methods are leveraged when low variance and high bias is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede5fa9",
   "metadata": {},
   "source": [
    "Random farest,adaoost,and gradientboost all use classification and regression both and use sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322976c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn     \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61995e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3706d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.data[:, :4]\n",
    "y=iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=.30, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add6c76",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9eb8ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  3 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.81      1.00      0.90        13\n",
      "           2       1.00      0.80      0.89        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf= RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "\n",
    "#n_estimatorsint, default=100 The number of trees in the forest.\n",
    "rf.fit(x_train,y_train)\n",
    "pred_rf=rf.predict(x_test)\n",
    "print(accuracy_score(y_test,pred_rf))\n",
    "print(confusion_matrix(y_test,pred_rf))\n",
    "print(classification_report(y_test,pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfaa28",
   "metadata": {},
   "source": [
    "# BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdfb2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  2 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.87      1.00      0.93        13\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    " from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ad= AdaBoostClassifier()\n",
    "\n",
    "#base estimator by default DFF and n_Estimater 50 by default\n",
    "ad.fit(x_train,y_train)\n",
    "pred_ad=ad.predict(x_test)\n",
    "print(accuracy_score(y_test,pred_ad))\n",
    "print(confusion_matrix(y_test,pred_ad))\n",
    "print(classification_report(y_test,pred_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9dd4e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  2 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.87      1.00      0.93        13\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad= AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "#base estimator by default DFF and n_Estimater 50 by default\n",
    "ad.fit(x_train,y_train)\n",
    "pred_ad=ad.predict(x_test)\n",
    "print(accuracy_score(y_test,pred_ad))\n",
    "print(confusion_matrix(y_test,pred_ad))\n",
    "print(classification_report(y_test,pred_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7b627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
